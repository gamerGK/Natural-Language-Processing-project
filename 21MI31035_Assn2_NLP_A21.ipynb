{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 20IE10049_Assn2_NLP_A21.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harsh16kh/NLP/blob/main/Copy_of_20IE10049_Assn2_NLP_A21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyJ25uz0kSaw"
      },
      "source": [
        "# **Assignment-2 for CS60075: Natural Language Processing**\n",
        "\n",
        "#### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\n",
        "\n",
        "#### Date of Announcement: 15th Sept, 2021\n",
        "#### Deadline for Submission: 11.59pm on Wednesday, 22nd Sept, 2021 \n",
        "#### Submit this .ipynb file, named as `<Your_Roll_Number>_Assn2_NLP_A21.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao1nhg9RknmF"
      },
      "source": [
        "The central idea of this assignment is to use Naive Bayes classifier and LSTM based classifier and compare the models by accuracy on IMDB dataset.  This dataset consists of 50k movie reviews (25k positive, 25k negative). You can download the dataset from https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONM5Q4SCe9Mr"
      },
      "source": [
        "Please submit with outputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElRkQElWUMjG"
      },
      "source": [
        "import re\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhHRim2AUm4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3016050-5f81-4fec-d91c-0ea0cb0226a4"
      },
      "source": [
        "#Load the IMDB dataset. You can load it using pandas as dataframe\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85b4SjZ7J0X3"
      },
      "source": [
        "#Read the dataset\n",
        "df = pd.read_csv('drive/MyDrive/IMDB_dataset/IMDB Dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK_Hn2f6VMP7"
      },
      "source": [
        "# Preprocessing\n",
        "PrePrecessing that needs to be done on lower cased corpus\n",
        "\n",
        "1. Remove html tags\n",
        "2. Remove URLS\n",
        "3. Remove non alphanumeric character\n",
        "4. Remove Stopwords\n",
        "5. Perform stemming and lemmatization\n",
        "\n",
        "You can use regex from re. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4pe9v8n3S0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ccaf60e-56e4-4326-8fe8-79282621cb0a"
      },
      "source": [
        "import nltk, math\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B5lHZPsVOXv"
      },
      "source": [
        "def apply1(text):\n",
        "    text = text.replace('<br />', ' ')\n",
        "    text = re.sub('http\\S+',' ',text)\n",
        "    return text\n",
        "\n",
        "L=[]\n",
        "def apply2(text):\n",
        "    for sent in sent_tokenize(text):\n",
        "        L.append(len(word_tokenize(sent)))\n",
        "    return text\n",
        "\n",
        "def apply3(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9 ]',' ',text)\n",
        "    text = text.lower()\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = list(set(words)-set(stopwords.words('english')))\n",
        "    text = ' '.join(filtered_words)\n",
        "    return text\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T22ZIWT_CZ1v"
      },
      "source": [
        "df['review']=df['review'].apply(apply1)\n",
        "df['review']=df['review'].apply(apply2)\n",
        "average = sum(L)/len(L)\n",
        "df['review']=df['review'].apply(apply3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV5ZOkkeAz3L"
      },
      "source": [
        "#Stemming\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "ps = PorterStemmer()\n",
        "le = WordNetLemmatizer()\n",
        "def lemmatization(text):\n",
        "    words = word_tokenize(text)\n",
        "    for i in range(len(words)):\n",
        "        words[i] = le.lemmatize(words[i])\n",
        "    text = words[0]\n",
        "    for i in range(1,len(words)):\n",
        "        text += ' '\n",
        "        text += words[i]\n",
        "    return text\n",
        "\n",
        "def stemming(text):\n",
        "    words = word_tokenize(text)\n",
        "    for i in range(len(words)):\n",
        "        words[i] = ps.stem(words[i])\n",
        "    text = words[0]\n",
        "    for i in range(1,len(words)):\n",
        "        text += ' '\n",
        "        text += words[i]\n",
        "    return text\n",
        "\n",
        "df['review'] = df['review'].apply(stemming)\n",
        "df['review'] = df['review'].apply(lemmatization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaSkfcvYGXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfcadde4-de1a-47b3-d39a-393473a4cbef"
      },
      "source": [
        "# Print Statistics of Data like avg length of sentence , proposition of data w.r.t class labels\n",
        "print('Average length of sentence: ', sum(L)/len(L))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average length of sentence:  21.454168404279315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTdX3yOL9KrQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "03cb204c-3b76-4d35-9e7a-77b36414a44a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>due becom one death punch first got happen acc...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>realli one got mural surfac actor refer littl ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>point one manag air decad averag crown right w...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>divorc life well becom watch instead expect th...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>one point habitat connect talent await situat ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  due becom one death punch first got happen acc...  positive\n",
              "1  realli one got mural surfac actor refer littl ...  positive\n",
              "2  point one manag air decad averag crown right w...  positive\n",
              "3  divorc life well becom watch instead expect th...  negative\n",
              "4  one point habitat connect talent await situat ...  positive"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVq-mN28U_J4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531ce4b1-45cf-4670-cb81-bbf1ddd42348"
      },
      "source": [
        "# get reviews column from df\n",
        "reviews = df['review']\n",
        "\n",
        "# get labels column from df\n",
        "labels = df['sentiment']\n",
        "reviews,labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0        due becom one death punch first got happen acc...\n",
              " 1        realli one got mural surfac actor refer littl ...\n",
              " 2        point one manag air decad averag crown right w...\n",
              " 3        divorc life well becom watch instead expect th...\n",
              " 4        one point habitat connect talent await situat ...\n",
              "                                ...                        \n",
              " 49995    soul life good becom one yr first 20 catwoman ...\n",
              " 49996    springtim sing stuck soundtrack beyond thousan...\n",
              " 49997    two author first wrong religion movi eye catho...\n",
              " 49998    one mumbl appropri explain scene park progress...\n",
              " 49999    good mccoy hardli well worst one spock chanc g...\n",
              " Name: review, Length: 50000, dtype: object, 0        positive\n",
              " 1        positive\n",
              " 2        positive\n",
              " 3        negative\n",
              " 4        positive\n",
              "            ...   \n",
              " 49995    positive\n",
              " 49996    negative\n",
              " 49997    negative\n",
              " 49998    negative\n",
              " 49999    negative\n",
              " Name: sentiment, Length: 50000, dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljo5NquhXTXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e04d20a-39b2-4b0e-de88-e03c7f7d3324"
      },
      "source": [
        "# Use label encoder to encode labels. Convert to 0/1\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "print(encoder.classes_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['negative' 'positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzG-C_EVWWET"
      },
      "source": [
        "# Split the data into train and test (80% - 20%). \n",
        "# Use stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels, test_size=0.2,random_state=42)\n",
        "# train_sentences, test_sentences, train_labels, test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlNql0acU7sa"
      },
      "source": [
        "# *LSTM* based Classifier\n",
        "\n",
        "Use the above train and test splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkqnvbUOXoN0"
      },
      "source": [
        "# Hyperparameters of the model\n",
        "vocab_size = 3000# choose based on statistics\n",
        "oov_tok = '<OOK>'\n",
        "embedding_dim = 100\n",
        "max_length = max([len(text) for text in reviews])# choose based on statistics, for example 150 to 200\n",
        "padding_type='post'\n",
        "trunc_type='post'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeycEg9nZAOF"
      },
      "source": [
        "# tokenize sentences\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# convert train dataset to sequence and pad sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
        "\n",
        "# convert Test dataset to sequence and pad sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtw3w895ZP39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a44c40-e314-4156-9319-de38bf3e71b5"
      },
      "source": [
        "# model initialization\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "    keras.layers.Dense(24, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 5722, 100)         300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               84480     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 24)                3096      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 387,601\n",
            "Trainable params: 387,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skmaDJMnZTzc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682ef0bc-4ea7-40b1-a1c2-f37b4dd34eca"
      },
      "source": [
        "num_epochs = 2 #reduced the number of epochs due to time constraints. The runtime in google colab gets disconnected after a certain point of time.\n",
        "history = model.fit(train_padded, train_labels, \n",
        "                    epochs=num_epochs, verbose=1, \n",
        "                    validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1125/1125 [==============================] - 1329s 1s/step - loss: 0.3756 - accuracy: 0.8344 - val_loss: 0.3104 - val_accuracy: 0.8720\n",
            "Epoch 2/2\n",
            "1125/1125 [==============================] - 1328s 1s/step - loss: 0.2772 - accuracy: 0.8882 - val_loss: 0.3077 - val_accuracy: 0.8773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjEhWEr5Zq7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d037bf98-eac9-4b19-abed-7a391c31782f"
      },
      "source": [
        "# Calculate accuracy on Test data\n",
        "prediction = model.predict(test_padded)\n",
        "\n",
        "# Get probabilities\n",
        "print('Probabilities: ', prediction)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities:  [[0.40844065]\n",
            " [0.8989985 ]\n",
            " [0.49020112]\n",
            " ...\n",
            " [0.90914536]\n",
            " [0.08338819]\n",
            " [0.60208064]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3s2PGHKCnMp",
        "outputId": "77c4ab79-ad41-49d0-ce59-854d24dc52af"
      },
      "source": [
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "labels = np.zeros((10000,1))\n",
        "for i in range(10000):\n",
        "  if prediction[i][0] > 0.5:\n",
        "    labels[i][0]=1\n",
        "c = 0\n",
        "for i in range(10000):\n",
        "  if labels[i] == test_labels[i]:\n",
        "    c += 1\n",
        "\n",
        "\n",
        "# Accuracy : one can use classification_report from sklearn\n",
        "print('Accuracy: ', c/100, '%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  87.14 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIICV-ySOYL0"
      },
      "source": [
        "## Get predictions for random examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2RmfNL3OYL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa9ab46-6022-4ed8-9919-98cf5a43fb34"
      },
      "source": [
        "# reviews on which we need to predict\n",
        "sentence = [\"The movie was very touching and heart whelming\", \n",
        "            \"I have never seen a terrible movie like this\", \n",
        "            \"the movie plot is terrible but it had good acting\"]\n",
        "\n",
        "# convert to a sequence\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "# pad the sequence\n",
        "padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
        "\n",
        "# Get probabilities\n",
        "p = model.predict(padded)\n",
        "print(p)\n",
        "\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "for i in range(3):\n",
        "  if p[i]>0.5:\n",
        "    print(sentence[i], ': Positive')\n",
        "  else:\n",
        "    print(sentence[i], ': Negative')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.49791795]\n",
            " [0.57001346]\n",
            " [0.43192655]]\n",
            "The movie was very touching and heart whelming : Negative\n",
            "I have never seen a terrible movie like this : Positive\n",
            "the movie plot is terrible but it had good acting : Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K91rM-GmDsMk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}